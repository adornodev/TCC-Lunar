{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates  as md\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import seaborn           as sns\n",
    "import datetime\n",
    "import time\n",
    "import collections\n",
    "import operator\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from pandas                   import concat\n",
    "from math                     import sqrt\n",
    "from IPython.display          import Image\n",
    "from glob                     import glob\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing    import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics          import confusion_matrix\n",
    "from keras.models             import Sequential\n",
    "from keras.layers             import Dense, LSTM, Dropout\n",
    "from keras.utils              import np_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directories      = [\"C:/Users/T-Gamer/Documents/Projetos - Git/FinalCourseAssignment/Dataset/20171203/\"]\n",
    "file_delimiter   = '\\t'\n",
    "\n",
    "\n",
    "window_size              = 0\n",
    "should_use_normalization = True\n",
    "\n",
    "pothole_output   = 1\n",
    "speedbump_output = 2\n",
    "default_output   = 0\n",
    "\n",
    "tilt_threshold   = 40\n",
    "n_reg_lag        = 60\n",
    "features         = ['accelerometer_X','accelerometer_Y','accelerometer_Z','output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files_from_directories(directories, filetype = \"csv\", pattern = \"*.\"):\n",
    "    files      = []\n",
    "    _pattern   = pattern + filetype\n",
    "    \n",
    "    try:\n",
    "        for directory in directories:\n",
    "            for dir,_,_ in os.walk(directory):\n",
    "                files.extend(glob(os.path.join(dir, _pattern)))\n",
    "    except:\n",
    "        print(\"\\Error to get files from directories. Do you do \\\"import os\\\" and \\\"from glob import glob\\\" ?\")\n",
    "        \n",
    "    return files\n",
    "\n",
    "\n",
    "def window_normalization(serie, window_size):\n",
    "    elems     = np.zeros(len(serie))\n",
    "    pos_begin = pos_end = 0\n",
    "    media     = std = 0.0\n",
    "    count     = 0\n",
    "    \n",
    "    while count <= (len(elems) - window_size):\n",
    "        \n",
    "        #Is the process at the end?\n",
    "        if (count + window_size == len(elems)):\n",
    "            aux = count - 1\n",
    "            while count < len(elems):\n",
    "                elems[count] = elems[aux]\n",
    "                count +=1\n",
    "            break\n",
    "            \n",
    "        pos_end = pos_begin + window_size\n",
    "        media   = np.mean(serie[pos_begin:pos_end])\n",
    "        std     = np.std (serie[pos_begin:pos_end])\n",
    "        \n",
    "        elems[pos_begin] += np.sqrt((serie[pos_end] - media)**2/std)\n",
    "\n",
    "        count     +=1\n",
    "        pos_begin +=1\n",
    "    \n",
    "    return elems.tolist()\n",
    "    \n",
    "    \n",
    "def full_normalization(serie):\n",
    "    return np.sqrt((serie - np.mean(serie))**2/np.std(serie))\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    \"\"\"\n",
    "    Frame a time series as a supervised learning dataset.\n",
    "    Arguments:\n",
    "        data: Sequence of observations as a list or NumPy array.\n",
    "        n_in: Number of lag observations as input (X).\n",
    "        n_out: Number of observations as output (y).\n",
    "        dropnan: Boolean whether or not to drop rows with NaN values.\n",
    "    Returns:\n",
    "        Pandas DataFrame of series framed for supervised learning.\n",
    "    \"\"\"\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "            \n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    \n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "        \n",
    "    return agg\n",
    "\n",
    "def string_operation_to_datetime(_datetimestr = \"\", _format = \"\", _milliseconds = 0, _seconds = 0, _minutes = 0, _hours = 0):\n",
    "    \n",
    "    # First case: Just sum values with current datetime\n",
    "    if _datetimestr is \"\" and _format is \"\":\n",
    "        current_date = datetime.datetime.strptime(datetime.datetime.today().strftime('%Y %m %d %H %M %S %f'), \"%Y %m %d %H %M %S %f\")\n",
    "        current_date = current_date + datetime.timedelta(hours = _hours, minutes = _minutes, seconds = _seconds, milliseconds = _milliseconds)\n",
    "        return str(current_date)\n",
    "    \n",
    "    # Second case: Sum values with given datetime\n",
    "    if _datetimestr is not \"\" and _format is not \"\":\n",
    "        try:\n",
    "            current_date = datetime.datetime.strptime(_datetimestr, _format)\n",
    "            current_date = current_date + datetime.timedelta(hours = _hours, minutes = _minutes, seconds = _seconds, milliseconds = _milliseconds)\n",
    "            return str(current_date)\n",
    "        except:\n",
    "            print('Are you sure you give the right format?')\n",
    "            return None\n",
    "        \n",
    "    print('This method have two possible cases.\\nFirst: _datetimestr and _format not filled.\\nSecond:_datetimestr and _format filled\\n')\n",
    "    return None\n",
    "\n",
    "def insert_repeated_values_on_dataframe(dataframe, column_name, index_list, value, seconds = 1, both_side = True, forward = True):\n",
    "    \n",
    "    list_timestamp = []\n",
    "    for dt in dataframe['datetime']:\n",
    "        list_timestamp.append(int(time.mktime(time.strptime(dt,'%Y-%m-%d %H:%M:%S.%f'))))\n",
    "    \n",
    "    df_general['datetime_timestamp'] = list_timestamp\n",
    "\n",
    "    for idx in index_list:\n",
    "        \n",
    "        if both_side:\n",
    "            begin_date = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] - seconds\n",
    "            end_date   = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] + seconds\n",
    "        else:\n",
    "            if forward:\n",
    "                begin_date = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] - 0\n",
    "                end_date   = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] + seconds\n",
    "            else:\n",
    "                begin_date = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] - seconds\n",
    "                end_date   = list((dataframe.loc[[idx]])['datetime_timestamp'].values)[0] + 0\n",
    "                \n",
    "        dataframe.loc[(dataframe['datetime_timestamp'] <= end_date) & (df_general['datetime_timestamp'] >= begin_date), column_name] = value\n",
    "            \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def convert_output_to_categorical_mode(y_train, y_test):\n",
    "    \n",
    "    outputs, result = list(), tuple()\n",
    "    \n",
    "    outputs.append(x_train)\n",
    "    outputs.append(x_test)\n",
    "\n",
    "    for output in outputs:\n",
    "        unique_sequence_out = []\n",
    "        for sequence in output: \n",
    "            counter = collections.Counter(list(sequence[:,-1]))\n",
    "            counter = dict(counter)\n",
    "\n",
    "            sorted_output_by_frequency = sorted(counter.items(), key=operator.itemgetter(1))\n",
    "            sorted_output_by_frequency.reverse()\n",
    "\n",
    "\n",
    "            if (sorted_output_by_frequency[0][0] == 0):\n",
    "                unique_sequence_out.append([1,0,0])\n",
    "            elif (sorted_output_by_frequency[0][0] == 1):\n",
    "                unique_sequence_out.append([0,1,0])\n",
    "            else:\n",
    "                unique_sequence_out.append([0,0,1])\n",
    "\n",
    "        if not result:\n",
    "            result = (np.asarray(unique_sequence_out),)\n",
    "        else:\n",
    "            result = result + (np.asarray(unique_sequence_out),)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def temporal_data_to_supervised_learning(features, n_reg_lag = 60):\n",
    "    \n",
    "    # Apply filter on general dataframe\n",
    "    values = df_general.filter(features, axis=1).values\n",
    "\n",
    "    # Specify the number of registers lag and n_features\n",
    "    # n_reg_lag = 60 means that i wanna process 60 values (2 seconds from past) from past to predict the current timestep\n",
    "    n_features = len(features)\n",
    "\n",
    "    # Frame as supervised learning\n",
    "    reframed = series_to_supervised(values, n_reg_lag, 1)\n",
    "\n",
    "    # Drop columns we don't want to predict\n",
    "    unnecessary_columns  = []\n",
    "    unnecessary_columns += [('var%d(t)' % (j+1)) for j in range(len(features) -1)]\n",
    "    reframed.drop(unnecessary_columns, axis=1, inplace=True)\n",
    "    \n",
    "    return reframed\n",
    "\n",
    "def split_data_to_training_and_test(dataframe, n_features, n_reg_lag, test_size = 0.2):\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    values = dataframe.values\n",
    "\n",
    "    n_obs = n_features * n_reg_lag\n",
    "\n",
    "    # Split data into training and test\n",
    "    x_train, x_test, y_train, y_test = train_test_split(values[:,:n_obs], values[:,[n_obs]], test_size=test_size, random_state=42)\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    x_train = x_train.reshape((x_train.shape[0], n_reg_lag, n_features))\n",
    "    x_test  = x_test.reshape((x_test.shape[0]  , n_reg_lag, n_features))\n",
    "\n",
    "    print(\"Train's data shape: {},{}\".format(x_train.shape, y_train.shape))\n",
    "    print(\"Test's  data shape: {},{}\".format(x_test.shape,  y_test.shape ))\n",
    "    \n",
    "    return (x_train, x_test, y_train, y_test)\n",
    "\n",
    "\n",
    "def build_confusion_matrix(y_test, yhat, labels = ['Default','Pothole', 'SpeedBump']):\n",
    "    \n",
    "    y_pred = np.argmax(yhat, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    classes = labels\n",
    "    \n",
    "    y_true = [classes[i] for i in y_true]\n",
    "    y_pred = [classes[i] for i in y_pred]\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, labels = classes)\n",
    "    \n",
    "    ax= plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, ax = ax, fmt='d');\n",
    "    sns.set(font_scale=2)\n",
    "\n",
    "    # Labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels') \n",
    "    ax.set_title('Confusion Matrix')\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    \n",
    "    \n",
    "def plot_tooltip_bokeh(axisSensor, dataframe, dataframe_pothole,dataframe_speedBump, _color = 'red', pos_df_pothole = 15, pos_df_speedBump = 18):\n",
    "    from bokeh.models   import HoverTool, BoxZoomTool,UndoTool, SaveTool\n",
    "    from bokeh.charts   import Line\n",
    "    from bokeh.plotting import output_notebook, show\n",
    "    from bokeh.models   import  BasicTickFormatter\n",
    "\n",
    "    output_notebook()\n",
    "    \n",
    "    data      = {axisSensor: dataframe[axisSensor].tolist()}\n",
    "    index     = dataframe['timestamp'].tolist()\n",
    "    dataframe = pd.DataFrame(data=data, index=index)\n",
    "    dataframe = dataframe.assign(x=dataframe.index)\n",
    "    columns   = dataframe.columns.values.tolist()\n",
    "    columns.remove('x')\n",
    "\n",
    "    dataframe_pothole['Pothole occurrence']     = pos_df_pothole\n",
    "    dataframe_speedBump['SpeedBump occurrence'] = pos_df_speedBump\n",
    "\n",
    "    hover = HoverTool(\n",
    "            tooltips=[\n",
    "                (\"lineNumberFile\", \"$index\"),\n",
    "                (\"Timestamp, accelerometer\", \"($x{int}, $y)\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Sensor's Data\n",
    "    p = Line(data = dataframe, x ='x', legend = False,\n",
    "             title=\"AXIS \" + axisSensor[-1] + \" - ACCELEROMETER\",\n",
    "             xlabel='Timestamp', ylabel='m/s^2',\n",
    "             width=950, height=400, color=_color,\n",
    "             notebook=True, tools = [hover,BoxZoomTool(),UndoTool(),SaveTool()])\n",
    "\n",
    "    p.y_range.end   = 25\n",
    "    p.x_range.start = dataframe['x'].iloc[0] - 10\n",
    "    p.x_range.end   = dataframe['x'].iloc[-1]+ 1000\n",
    "\n",
    "    # Pothole\n",
    "    p.circle(dataframe_pothole['timestamp'].tolist(), dataframe_pothole['Pothole occurrence'].tolist(),fill_color=\"white\",size=8)\n",
    "\n",
    "    # SpeedBump\n",
    "    p.inverted_triangle(dataframe_speedBump['timestamp'].tolist(), dataframe_speedBump['SpeedBump occurrence'].tolist(),size=8, color=\"#DE2D26\")\n",
    "\n",
    "    p.xaxis.formatter=BasicTickFormatter(use_scientific=False)\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train's data shape: (10777, 60, 4),(10777, 1)\n",
      "Test's  data shape: (2695, 60, 4),(2695, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get Paths from specific file\n",
    "speedbumpPath = get_files_from_directories(directories, pattern = \"SpeedBump_Postognv*.\")[0]\n",
    "potholePath   = get_files_from_directories(directories, pattern = \"Pothole_Postognv*.\")[0]\n",
    "generalPath   = get_files_from_directories(directories, pattern = \"Postognv*.\")[0]\n",
    "#speedbumpPath = get_files_from_directories(directoriesList, pattern = \"SpeedBump_Inicioasfaltovelhoaterodovia*.\")[0]\n",
    "#potholePath   = get_files_from_directories(directoriesList, pattern = \"Pothole_Inicioasfaltovelhoaterodovia*.\")[0]\n",
    "#generalPath   = get_files_from_directories(directoriesList, pattern = \"Inicioasfaltovelhoaterodovia*.\")[0]\n",
    "\n",
    "\n",
    "# Reading csv files\n",
    "df_general    = pd.read_csv(generalPath, delimiter   = file_delimiter)\n",
    "df_pothole    = pd.read_csv(potholePath, delimiter   = file_delimiter)\n",
    "df_speedbump  = pd.read_csv(speedbumpPath, delimiter = file_delimiter)\n",
    "\n",
    "\n",
    "# Create \"output\" column\n",
    "df_general.loc[df_general['timestamp'].isin(list(df_pothole['timestamp']))  , 'output'] = pothole_output\n",
    "df_general.loc[df_general['timestamp'].isin(list(df_speedbump['timestamp'])), 'output'] = speedbump_output\n",
    "\n",
    "\n",
    "# Put default_output on rows with NaN values\n",
    "df_general['output'] = df_general['output'].fillna(default_output)\n",
    "\n",
    "\n",
    "# Add the values from \"timestamp\" column in the current datetime and a new datetime column is created to be used as index\n",
    "datetime_values = []\n",
    "for value in list(df_general['timestamp']):\n",
    "    result = string_operation_to_datetime(_milliseconds = int(value))\n",
    "    if result is not None:\n",
    "        datetime_values.append(result)\n",
    "df_general['datetime'] = datetime_values\n",
    "df_general.set_index('datetime')\n",
    "\n",
    "\n",
    "# Add event's values in a range of seconds in the dataframe\n",
    "df_general = insert_repeated_values_on_dataframe(df_general, 'output', list(df_general[df_general['output'] == pothole_output].index)  , pothole_output,   seconds = 1, both_side = True)\n",
    "df_general = insert_repeated_values_on_dataframe(df_general, 'output', list(df_general[df_general['output'] == speedbump_output].index), speedbump_output, seconds = 1, both_side = True)\n",
    "\n",
    "\n",
    "# Remove dataframe's rows with high tilt\n",
    "df_general = df_general[(df_general['tilt'] > tilt_threshold)]\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "if should_use_normalization:\n",
    "    if window_size < 1:\n",
    "        df_general['accelerometer_X'] = full_normalization(df_general['accelerometer_X'])\n",
    "        df_general['accelerometer_Y'] = full_normalization(df_general['accelerometer_Y'])\n",
    "        df_general['accelerometer_Z'] = full_normalization(df_general['accelerometer_Z'])\n",
    "    else:\n",
    "        df_general['accelerometer_X'] = window_normalization(df_general['accelerometer_X'], window_size)\n",
    "        df_general['accelerometer_Y'] = window_normalization(df_general['accelerometer_Y'], window_size)\n",
    "        df_general['accelerometer_Z'] = window_normalization(df_general['accelerometer_Z'], window_size)\n",
    "        \n",
    "\n",
    "# Convert temporal data to supervised problem\n",
    "reframed = temporal_data_to_supervised_learning(features, n_reg_lag)\n",
    "\n",
    "\n",
    "# Split data into training and test arrays\n",
    "x_train, x_test, y_train, y_test = split_data_to_training_and_test(reframed, len(features), n_reg_lag)\n",
    "\n",
    "\n",
    "# Convert output to categorical mode\n",
    "y_train, y_test = convert_output_to_categorical_mode(y_train, y_test)\n",
    "\n",
    "\n",
    "# Remove target column from x_train and x_test\n",
    "x_train = x_train[:, :, :-1]\n",
    "x_test  = x_test[:, :, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plotTooltipBokeh('accelerometer_Y', df_general, df_pothole,df_speedbump, 'red', 15, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               41600     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 41,903\n",
      "Trainable params: 41,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 10777 samples, validate on 2695 samples\n",
      "Epoch 1/200\n",
      "10777/10777 [==============================] - 8s 742us/step - loss: 0.4503 - acc: 0.8811 - val_loss: 0.2957 - val_acc: 0.8961\n",
      "Epoch 2/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.3171 - acc: 0.8930 - val_loss: 0.2787 - val_acc: 0.9061\n",
      "Epoch 3/200\n",
      "10777/10777 [==============================] - 7s 622us/step - loss: 0.2785 - acc: 0.8989 - val_loss: 0.2594 - val_acc: 0.9017\n",
      "Epoch 4/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.2513 - acc: 0.9110 - val_loss: 0.2087 - val_acc: 0.9302\n",
      "Epoch 5/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.2350 - acc: 0.9225 - val_loss: 0.2039 - val_acc: 0.9317\n",
      "Epoch 6/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.2242 - acc: 0.9239 - val_loss: 0.1866 - val_acc: 0.9395\n",
      "Epoch 7/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.2133 - acc: 0.9269 - val_loss: 0.1782 - val_acc: 0.9369\n",
      "Epoch 8/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.2039 - acc: 0.9292 - val_loss: 0.1639 - val_acc: 0.9414\n",
      "Epoch 9/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.2081 - acc: 0.9280 - val_loss: 0.1740 - val_acc: 0.9351\n",
      "Epoch 10/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.1912 - acc: 0.9326 - val_loss: 0.1514 - val_acc: 0.9473\n",
      "Epoch 11/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.1781 - acc: 0.9358 - val_loss: 0.1461 - val_acc: 0.9506\n",
      "Epoch 12/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.1776 - acc: 0.9384 - val_loss: 0.1514 - val_acc: 0.9477\n",
      "Epoch 13/200\n",
      "10777/10777 [==============================] - 7s 618us/step - loss: 0.1645 - acc: 0.9440 - val_loss: 0.1423 - val_acc: 0.9484\n",
      "Epoch 14/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.1495 - acc: 0.9497 - val_loss: 0.1245 - val_acc: 0.9603\n",
      "Epoch 15/200\n",
      "10777/10777 [==============================] - 7s 610us/step - loss: 0.1381 - acc: 0.9516 - val_loss: 0.1210 - val_acc: 0.9525\n",
      "Epoch 16/200\n",
      "10777/10777 [==============================] - 7s 620us/step - loss: 0.1349 - acc: 0.9515 - val_loss: 0.1475 - val_acc: 0.9443\n",
      "Epoch 17/200\n",
      "10777/10777 [==============================] - 7s 612us/step - loss: 0.1335 - acc: 0.9514 - val_loss: 0.0904 - val_acc: 0.9692\n",
      "Epoch 18/200\n",
      "10777/10777 [==============================] - 7s 620us/step - loss: 0.1078 - acc: 0.9621 - val_loss: 0.0874 - val_acc: 0.9640\n",
      "Epoch 19/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0993 - acc: 0.9660 - val_loss: 0.0814 - val_acc: 0.9744\n",
      "Epoch 20/200\n",
      "10777/10777 [==============================] - 7s 625us/step - loss: 0.1151 - acc: 0.9580 - val_loss: 0.0977 - val_acc: 0.9692\n",
      "Epoch 21/200\n",
      "10777/10777 [==============================] - 7s 626us/step - loss: 0.0913 - acc: 0.9685 - val_loss: 0.0846 - val_acc: 0.9722\n",
      "Epoch 22/200\n",
      "10777/10777 [==============================] - 7s 613us/step - loss: 0.0737 - acc: 0.9733 - val_loss: 0.0686 - val_acc: 0.9781\n",
      "Epoch 23/200\n",
      "10777/10777 [==============================] - 7s 615us/step - loss: 0.0787 - acc: 0.9742 - val_loss: 0.0654 - val_acc: 0.9788\n",
      "Epoch 24/200\n",
      "10777/10777 [==============================] - 7s 614us/step - loss: 0.0649 - acc: 0.9789 - val_loss: 0.0466 - val_acc: 0.9866\n",
      "Epoch 25/200\n",
      "10777/10777 [==============================] - 7s 610us/step - loss: 0.0544 - acc: 0.9811 - val_loss: 0.0673 - val_acc: 0.9766\n",
      "Epoch 26/200\n",
      "10777/10777 [==============================] - 7s 610us/step - loss: 0.0644 - acc: 0.9789 - val_loss: 0.0575 - val_acc: 0.9818\n",
      "Epoch 27/200\n",
      "10777/10777 [==============================] - 7s 617us/step - loss: 0.0439 - acc: 0.9849 - val_loss: 0.0389 - val_acc: 0.9870\n",
      "Epoch 28/200\n",
      "10777/10777 [==============================] - 7s 608us/step - loss: 0.0419 - acc: 0.9860 - val_loss: 0.0341 - val_acc: 0.9911\n",
      "Epoch 29/200\n",
      "10777/10777 [==============================] - 7s 618us/step - loss: 0.0515 - acc: 0.9847 - val_loss: 0.0540 - val_acc: 0.9829\n",
      "Epoch 30/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0364 - acc: 0.9877 - val_loss: 0.0434 - val_acc: 0.9881\n",
      "Epoch 31/200\n",
      "10777/10777 [==============================] - 7s 616us/step - loss: 0.0467 - acc: 0.9852 - val_loss: 0.0464 - val_acc: 0.9859\n",
      "Epoch 32/200\n",
      "10777/10777 [==============================] - 7s 627us/step - loss: 0.0389 - acc: 0.9869 - val_loss: 0.0307 - val_acc: 0.9907\n",
      "Epoch 33/200\n",
      "10777/10777 [==============================] - 7s 619us/step - loss: 0.0369 - acc: 0.9873 - val_loss: 0.1998 - val_acc: 0.9469\n",
      "Epoch 34/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0617 - acc: 0.9791 - val_loss: 0.0468 - val_acc: 0.9837\n",
      "Epoch 35/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0307 - acc: 0.9888 - val_loss: 0.0264 - val_acc: 0.9915\n",
      "Epoch 36/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0378 - acc: 0.9880 - val_loss: 0.0503 - val_acc: 0.9826\n",
      "Epoch 37/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0314 - acc: 0.9902 - val_loss: 0.0233 - val_acc: 0.9922\n",
      "Epoch 38/200\n",
      "10777/10777 [==============================] - 7s 619us/step - loss: 0.0217 - acc: 0.9918 - val_loss: 0.0238 - val_acc: 0.9918\n",
      "Epoch 39/200\n",
      "10777/10777 [==============================] - 7s 625us/step - loss: 0.0276 - acc: 0.9905 - val_loss: 0.0209 - val_acc: 0.9926\n",
      "Epoch 40/200\n",
      "10777/10777 [==============================] - 7s 631us/step - loss: 0.0238 - acc: 0.9927 - val_loss: 0.0338 - val_acc: 0.9878\n",
      "Epoch 41/200\n",
      "10777/10777 [==============================] - 7s 631us/step - loss: 0.0250 - acc: 0.9915 - val_loss: 0.0223 - val_acc: 0.9926\n",
      "Epoch 42/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0196 - acc: 0.9933 - val_loss: 0.0408 - val_acc: 0.9885\n",
      "Epoch 43/200\n",
      "10777/10777 [==============================] - 7s 621us/step - loss: 0.0263 - acc: 0.9915 - val_loss: 0.0157 - val_acc: 0.9929\n",
      "Epoch 44/200\n",
      "10777/10777 [==============================] - 7s 610us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.0279 - val_acc: 0.9900\n",
      "Epoch 45/200\n",
      "10777/10777 [==============================] - 7s 617us/step - loss: 0.0142 - acc: 0.9951 - val_loss: 0.0210 - val_acc: 0.9922\n",
      "Epoch 46/200\n",
      "10777/10777 [==============================] - 7s 625us/step - loss: 0.0210 - acc: 0.9930 - val_loss: 0.0220 - val_acc: 0.9933\n",
      "Epoch 47/200\n",
      "10777/10777 [==============================] - 7s 613us/step - loss: 0.0201 - acc: 0.9927 - val_loss: 0.0293 - val_acc: 0.9885\n",
      "Epoch 48/200\n",
      "10777/10777 [==============================] - 7s 622us/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0138 - val_acc: 0.9952\n",
      "Epoch 49/200\n",
      "10777/10777 [==============================] - 7s 624us/step - loss: 0.0147 - acc: 0.9945 - val_loss: 0.0171 - val_acc: 0.9955\n",
      "Epoch 50/200\n",
      "10777/10777 [==============================] - 7s 626us/step - loss: 0.0178 - acc: 0.9928 - val_loss: 0.0219 - val_acc: 0.9933\n",
      "Epoch 51/200\n",
      "10777/10777 [==============================] - 7s 631us/step - loss: 0.0134 - acc: 0.9953 - val_loss: 0.0084 - val_acc: 0.9955\n",
      "Epoch 52/200\n",
      "10777/10777 [==============================] - 7s 616us/step - loss: 0.0122 - acc: 0.9955 - val_loss: 0.0194 - val_acc: 0.9944\n",
      "Epoch 53/200\n",
      "10777/10777 [==============================] - 7s 632us/step - loss: 0.0112 - acc: 0.9958 - val_loss: 0.0098 - val_acc: 0.9959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200\n",
      "10777/10777 [==============================] - 7s 622us/step - loss: 0.0152 - acc: 0.9949 - val_loss: 0.0108 - val_acc: 0.9952\n",
      "Epoch 55/200\n",
      "10777/10777 [==============================] - 7s 608us/step - loss: 0.0094 - acc: 0.9967 - val_loss: 0.0255 - val_acc: 0.9929\n",
      "Epoch 56/200\n",
      "10777/10777 [==============================] - 7s 615us/step - loss: 0.0082 - acc: 0.9967 - val_loss: 0.0119 - val_acc: 0.9955\n",
      "Epoch 57/200\n",
      "10777/10777 [==============================] - 7s 621us/step - loss: 0.0186 - acc: 0.9936 - val_loss: 0.0136 - val_acc: 0.9963\n",
      "Epoch 58/200\n",
      "10777/10777 [==============================] - 7s 622us/step - loss: 0.0116 - acc: 0.9956 - val_loss: 0.0219 - val_acc: 0.9918\n",
      "Epoch 59/200\n",
      "10777/10777 [==============================] - 7s 620us/step - loss: 0.0204 - acc: 0.9924 - val_loss: 0.0822 - val_acc: 0.9852\n",
      "Epoch 60/200\n",
      "10777/10777 [==============================] - 7s 614us/step - loss: 0.0259 - acc: 0.9906 - val_loss: 0.0191 - val_acc: 0.9944\n",
      "Epoch 61/200\n",
      "10777/10777 [==============================] - 7s 618us/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.0184 - val_acc: 0.9944\n",
      "Epoch 62/200\n",
      "10777/10777 [==============================] - 7s 617us/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0283 - val_acc: 0.9941\n",
      "Epoch 63/200\n",
      "10777/10777 [==============================] - 7s 614us/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.0151 - val_acc: 0.9941\n",
      "Epoch 64/200\n",
      "10777/10777 [==============================] - 7s 608us/step - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0151 - val_acc: 0.9952\n",
      "Epoch 65/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0086 - acc: 0.9967 - val_loss: 0.0132 - val_acc: 0.9955\n",
      "Epoch 66/200\n",
      "10777/10777 [==============================] - 7s 611us/step - loss: 0.0089 - acc: 0.9962 - val_loss: 0.0092 - val_acc: 0.9967\n",
      "Epoch 67/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0134 - acc: 0.9950 - val_loss: 0.0106 - val_acc: 0.9955\n",
      "Epoch 68/200\n",
      "10777/10777 [==============================] - 7s 632us/step - loss: 0.0154 - acc: 0.9946 - val_loss: 0.0450 - val_acc: 0.9852\n",
      "Epoch 69/200\n",
      "10777/10777 [==============================] - 7s 631us/step - loss: 0.0229 - acc: 0.9921 - val_loss: 0.0101 - val_acc: 0.9955\n",
      "Epoch 70/200\n",
      "10777/10777 [==============================] - 7s 623us/step - loss: 0.0105 - acc: 0.9968 - val_loss: 0.0085 - val_acc: 0.9967\n",
      "Epoch 71/200\n",
      "10777/10777 [==============================] - 7s 627us/step - loss: 0.0072 - acc: 0.9974 - val_loss: 0.0055 - val_acc: 0.9981\n",
      "Epoch 72/200\n",
      "10777/10777 [==============================] - 7s 635us/step - loss: 0.0096 - acc: 0.9965 - val_loss: 0.0108 - val_acc: 0.9967\n",
      "Epoch 73/200\n",
      "10777/10777 [==============================] - 7s 624us/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0074 - val_acc: 0.9974\n",
      "Epoch 74/200\n",
      "10777/10777 [==============================] - 7s 615us/step - loss: 0.0072 - acc: 0.9968 - val_loss: 0.0158 - val_acc: 0.9963\n",
      "Epoch 75/200\n",
      "10777/10777 [==============================] - 7s 616us/step - loss: 0.0104 - acc: 0.9962 - val_loss: 0.0341 - val_acc: 0.9904\n",
      "Epoch 76/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0440 - acc: 0.9856 - val_loss: 0.0326 - val_acc: 0.9889\n",
      "Epoch 77/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0192 - acc: 0.9932 - val_loss: 0.0127 - val_acc: 0.9963\n",
      "Epoch 78/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0112 - acc: 0.9957 - val_loss: 0.0112 - val_acc: 0.9952\n",
      "Epoch 79/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0067 - acc: 0.9971 - val_loss: 0.0102 - val_acc: 0.9963\n",
      "Epoch 80/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0061 - acc: 0.9976 - val_loss: 0.0106 - val_acc: 0.9959\n",
      "Epoch 81/200\n",
      "10777/10777 [==============================] - 7s 603us/step - loss: 0.0072 - acc: 0.9972 - val_loss: 0.0275 - val_acc: 0.9907\n",
      "Epoch 82/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0212 - acc: 0.9931 - val_loss: 0.0290 - val_acc: 0.9911\n",
      "Epoch 83/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0091 - acc: 0.9963 - val_loss: 0.0142 - val_acc: 0.9948\n",
      "Epoch 84/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0092 - acc: 0.9970 - val_loss: 0.0093 - val_acc: 0.9967\n",
      "Epoch 85/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0073 - acc: 0.9969 - val_loss: 0.0067 - val_acc: 0.9970\n",
      "Epoch 86/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0048 - acc: 0.9981 - val_loss: 0.0086 - val_acc: 0.9967\n",
      "Epoch 87/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0050 - acc: 0.9981 - val_loss: 0.0092 - val_acc: 0.9959\n",
      "Epoch 88/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0119 - acc: 0.9966 - val_loss: 0.0175 - val_acc: 0.9933\n",
      "Epoch 89/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0086 - val_acc: 0.9955\n",
      "Epoch 90/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0068 - acc: 0.9974 - val_loss: 0.0097 - val_acc: 0.9963\n",
      "Epoch 91/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0046 - acc: 0.9978 - val_loss: 0.0088 - val_acc: 0.9955\n",
      "Epoch 92/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0131 - val_acc: 0.9959\n",
      "Epoch 93/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0160 - val_acc: 0.9948\n",
      "Epoch 94/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0078 - acc: 0.9978 - val_loss: 0.0258 - val_acc: 0.9952\n",
      "Epoch 95/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0124 - acc: 0.9957 - val_loss: 0.0179 - val_acc: 0.9952\n",
      "Epoch 96/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0110 - val_acc: 0.9952\n",
      "Epoch 97/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0064 - acc: 0.9977 - val_loss: 0.0112 - val_acc: 0.9967\n",
      "Epoch 98/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0052 - acc: 0.9975 - val_loss: 0.0179 - val_acc: 0.9963\n",
      "Epoch 99/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0269 - acc: 0.9914 - val_loss: 0.0102 - val_acc: 0.9955\n",
      "Epoch 100/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0142 - acc: 0.9950 - val_loss: 0.0101 - val_acc: 0.9959\n",
      "Epoch 101/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0132 - acc: 0.9954 - val_loss: 0.0157 - val_acc: 0.9937\n",
      "Epoch 102/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.0063 - val_acc: 0.9959\n",
      "Epoch 103/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.0074 - val_acc: 0.9981\n",
      "Epoch 104/200\n",
      "10777/10777 [==============================] - 7s 603us/step - loss: 0.0052 - acc: 0.9978 - val_loss: 0.0056 - val_acc: 0.9970\n",
      "Epoch 105/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0043 - acc: 0.9984 - val_loss: 0.0083 - val_acc: 0.9963\n",
      "Epoch 106/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0052 - acc: 0.9979 - val_loss: 0.0139 - val_acc: 0.9959\n",
      "Epoch 107/200\n",
      "10777/10777 [==============================] - 7s 608us/step - loss: 0.0060 - acc: 0.9978 - val_loss: 0.0093 - val_acc: 0.9967\n",
      "Epoch 108/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0084 - val_acc: 0.9959\n",
      "Epoch 109/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.0092 - val_acc: 0.9959\n",
      "Epoch 110/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0106 - val_acc: 0.9959\n",
      "Epoch 111/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0175 - acc: 0.9942 - val_loss: 0.0091 - val_acc: 0.9959\n",
      "Epoch 112/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0087 - acc: 0.9970 - val_loss: 0.0086 - val_acc: 0.9970\n",
      "Epoch 113/200\n",
      "10777/10777 [==============================] - 7s 612us/step - loss: 0.0039 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9967\n",
      "Epoch 114/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0040 - acc: 0.9985 - val_loss: 0.0058 - val_acc: 0.9974\n",
      "Epoch 115/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0134 - val_acc: 0.9963\n",
      "Epoch 116/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0036 - acc: 0.9985 - val_loss: 0.0141 - val_acc: 0.9955\n",
      "Epoch 117/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0085 - acc: 0.9968 - val_loss: 0.0063 - val_acc: 0.9970\n",
      "Epoch 118/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0051 - acc: 0.9980 - val_loss: 0.0077 - val_acc: 0.9963\n",
      "Epoch 119/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0048 - acc: 0.9985 - val_loss: 0.0088 - val_acc: 0.9967\n",
      "Epoch 120/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0154 - val_acc: 0.9952\n",
      "Epoch 121/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0059 - acc: 0.9979 - val_loss: 0.0094 - val_acc: 0.9970\n",
      "Epoch 122/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0053 - acc: 0.9982 - val_loss: 0.0148 - val_acc: 0.9948\n",
      "Epoch 123/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0081 - acc: 0.9971 - val_loss: 0.0247 - val_acc: 0.9937\n",
      "Epoch 124/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 0.0174 - val_acc: 0.9948\n",
      "Epoch 125/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0099 - val_acc: 0.9948\n",
      "Epoch 126/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0082 - val_acc: 0.9963\n",
      "Epoch 127/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0068 - val_acc: 0.9967\n",
      "Epoch 128/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0100 - val_acc: 0.9959\n",
      "Epoch 129/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0070 - acc: 0.9979 - val_loss: 0.0174 - val_acc: 0.9944\n",
      "Epoch 130/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0058 - acc: 0.9981 - val_loss: 0.0085 - val_acc: 0.9955\n",
      "Epoch 131/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0096 - val_acc: 0.9959\n",
      "Epoch 132/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0049 - acc: 0.9982 - val_loss: 0.0066 - val_acc: 0.9970\n",
      "Epoch 133/200\n",
      "10777/10777 [==============================] - 7s 613us/step - loss: 0.0037 - acc: 0.9985 - val_loss: 0.0071 - val_acc: 0.9978\n",
      "Epoch 134/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0083 - acc: 0.9972 - val_loss: 0.0085 - val_acc: 0.9974\n",
      "Epoch 135/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0055 - acc: 0.9978 - val_loss: 0.0096 - val_acc: 0.9967\n",
      "Epoch 136/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0154 - val_acc: 0.9948\n",
      "Epoch 137/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.0058 - val_acc: 0.9978\n",
      "Epoch 138/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0078 - val_acc: 0.9967\n",
      "Epoch 139/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0098 - val_acc: 0.9955\n",
      "Epoch 140/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0098 - val_acc: 0.9967\n",
      "Epoch 141/200\n",
      "10777/10777 [==============================] - 7s 609us/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0102 - val_acc: 0.9967\n",
      "Epoch 142/200\n",
      "10777/10777 [==============================] - 7s 603us/step - loss: 0.0041 - acc: 0.9984 - val_loss: 0.0125 - val_acc: 0.9978\n",
      "Epoch 143/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0053 - val_acc: 0.9974\n",
      "Epoch 144/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0128 - val_acc: 0.9952\n",
      "Epoch 145/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0137 - acc: 0.9957 - val_loss: 0.0255 - val_acc: 0.9929\n",
      "Epoch 146/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.0125 - val_acc: 0.9955\n",
      "Epoch 147/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0051 - acc: 0.9981 - val_loss: 0.0091 - val_acc: 0.9963\n",
      "Epoch 148/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0082 - val_acc: 0.9963\n",
      "Epoch 149/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0098 - val_acc: 0.9963\n",
      "Epoch 150/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0106 - val_acc: 0.9959\n",
      "Epoch 151/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0118 - val_acc: 0.9959\n",
      "Epoch 152/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0026 - acc: 0.9990 - val_loss: 0.0129 - val_acc: 0.9963\n",
      "Epoch 153/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0034 - acc: 0.9983 - val_loss: 0.0124 - val_acc: 0.9963\n",
      "Epoch 154/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0075 - acc: 0.9973 - val_loss: 0.0126 - val_acc: 0.9948\n",
      "Epoch 155/200\n",
      "10777/10777 [==============================] - 6s 597us/step - loss: 0.0045 - acc: 0.9981 - val_loss: 0.0157 - val_acc: 0.9952\n",
      "Epoch 156/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0194 - val_acc: 0.9959\n",
      "Epoch 157/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0159 - acc: 0.9944 - val_loss: 0.0104 - val_acc: 0.9959\n",
      "Epoch 158/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0086 - acc: 0.9970 - val_loss: 0.0090 - val_acc: 0.9967\n",
      "Epoch 159/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0134 - val_acc: 0.9967\n",
      "Epoch 160/200\n",
      "10777/10777 [==============================] - 7s 607us/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0116 - val_acc: 0.9963\n",
      "Epoch 161/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0024 - acc: 0.9991 - val_loss: 0.0133 - val_acc: 0.9959\n",
      "Epoch 162/200\n",
      "10777/10777 [==============================] - 7s 605us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0143 - val_acc: 0.9963\n",
      "Epoch 163/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0108 - val_acc: 0.9970\n",
      "Epoch 164/200\n",
      "10777/10777 [==============================] - 6s 597us/step - loss: 0.0018 - acc: 0.9992 - val_loss: 0.0129 - val_acc: 0.9970\n",
      "Epoch 165/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0119 - acc: 0.9963 - val_loss: 0.0111 - val_acc: 0.9955\n",
      "Epoch 166/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0092 - acc: 0.9972 - val_loss: 0.0145 - val_acc: 0.9955\n",
      "Epoch 167/200\n",
      "10777/10777 [==============================] - 6s 598us/step - loss: 0.0040 - acc: 0.9988 - val_loss: 0.0076 - val_acc: 0.9970\n",
      "Epoch 168/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.0111 - val_acc: 0.9963\n",
      "Epoch 169/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0100 - val_acc: 0.9970\n",
      "Epoch 170/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0142 - val_acc: 0.9952\n",
      "Epoch 171/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0170 - val_acc: 0.9959\n",
      "Epoch 172/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0014 - acc: 0.9994 - val_loss: 0.0147 - val_acc: 0.9970\n",
      "Epoch 173/200\n",
      "10777/10777 [==============================] - 6s 598us/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0169 - val_acc: 0.9959\n",
      "Epoch 174/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0181 - val_acc: 0.9952\n",
      "Epoch 175/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0105 - val_acc: 0.9970\n",
      "Epoch 176/200\n",
      "10777/10777 [==============================] - 6s 598us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0112 - val_acc: 0.9959\n",
      "Epoch 177/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0153 - val_acc: 0.9963\n",
      "Epoch 178/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0271 - acc: 0.9933 - val_loss: 0.0088 - val_acc: 0.9959\n",
      "Epoch 179/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0066 - acc: 0.9977 - val_loss: 0.0108 - val_acc: 0.9963\n",
      "Epoch 180/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0342 - val_acc: 0.9926\n",
      "Epoch 181/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0117 - val_acc: 0.9955\n",
      "Epoch 182/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0108 - val_acc: 0.9959\n",
      "Epoch 183/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0111 - val_acc: 0.9959\n",
      "Epoch 184/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0078 - val_acc: 0.9967\n",
      "Epoch 185/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0085 - val_acc: 0.9967\n",
      "Epoch 186/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0075 - val_acc: 0.9967\n",
      "Epoch 187/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0011 - acc: 0.9998 - val_loss: 0.0069 - val_acc: 0.9967\n",
      "Epoch 188/200\n",
      "10777/10777 [==============================] - 7s 604us/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0069 - val_acc: 0.9970\n",
      "Epoch 189/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0063 - val_acc: 0.9970\n",
      "Epoch 190/200\n",
      "10777/10777 [==============================] - 6s 601us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.0080 - val_acc: 0.9967\n",
      "Epoch 191/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0076 - val_acc: 0.9967\n",
      "Epoch 192/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0013 - acc: 0.9993 - val_loss: 0.0081 - val_acc: 0.9967\n",
      "Epoch 193/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0087 - val_acc: 0.9963\n",
      "Epoch 194/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0105 - val_acc: 0.9963\n",
      "Epoch 195/200\n",
      "10777/10777 [==============================] - 6s 602us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.0091 - val_acc: 0.9970\n",
      "Epoch 196/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 7.7428e-04 - acc: 0.9997 - val_loss: 0.0101 - val_acc: 0.9963\n",
      "Epoch 197/200\n",
      "10777/10777 [==============================] - 6s 603us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0085 - val_acc: 0.9970\n",
      "Epoch 198/200\n",
      "10777/10777 [==============================] - 6s 600us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0152 - val_acc: 0.9952\n",
      "Epoch 199/200\n",
      "10777/10777 [==============================] - 6s 599us/step - loss: 0.0078 - acc: 0.9977 - val_loss: 0.0436 - val_acc: 0.9907\n",
      "Epoch 200/200\n",
      "10777/10777 [==============================] - 7s 606us/step - loss: 0.0055 - acc: 0.9986 - val_loss: 0.0067 - val_acc: 0.9974\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+iAJdyGE6IMk3IUQog+ScBdCiD7o/wPRTYsE0BvJUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x297366c7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(100,input_shape=(x_train.shape[1],x_train.shape[2])))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "\n",
    "#fit network\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=100, validation_data=(x_test, y_test), verbose=1, shuffle=False)\n",
    "\n",
    "# Plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Result from machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.74%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEYCAYAAACuiRYLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XecHVXdx/HPN9kUIKFJDaEJUqVI\nBwMJSi8iooCCSHkSelfxIZSoFEWKICUExIBKR4ogEFookRYBEaQ8FEE6gXRI2d3f88eZhctmN3t3\n987eu7Pfd17zujNzz8w5M3vzu+eeOXNGEYGZmXVvvapdADMz6zwHczOzAnAwNzMrAAdzM7MCcDA3\nMysAB3MzswJwMK9Rkr4t6VZJb0qaI+l9STdLGtqFZRgq6Z+SZkn6UNLyOeQxXlJ9pfdbZt5jJUU2\nbT2fdMtLaszSndSJ/FYvM92wzuZlPU9dtQtgXyRpEeAq4FvAE8AY4H1gJeBAYLykoyPigpzL0Qu4\nHhgAnAxMA97KIavTgSVz2G97fQ94oJX39gTUmZ1LOpF0HhcoI/kLwA+BZzqTp/Us8k1DtUXSTcB3\ngHkCdhboHwLWBbaJiPtyLMcg4G3gxoj4Xl75VJOkscCPgFdJX1qDIqKxhXRPACsCSwEnR8RpHchr\nPDAkIlyBsly4maWGSNqRFMivaanmHRFTgUOzxSNzLk7f7HVqzvnUghuApYEtm78haWVgY+DGri6U\nWXs4mNeWH2avF7WWICL+DnyVFPQ/k7Vv3ylpStbG/S9Jx0rqXZJmpawt9hhJR0p6QdJsSW9IOk1S\nnyzdWOD1bLODsm1GlWx/efNytdT2LemHkp6UNFXSDElPSDqojO2WkPS7rFxzJL0j6QpJK7Sw7TOS\nNpR0j6TpWV43S1qttXPYghuBRuC7Lby3FzATuL2lDSXtLel+SR9Lmptd27hO0qolaQIYCvTOzt/Y\nZuU/TNKk7Bz9pHmbuaRTsuVLmuV9dLb+/HYcqxVVRHiqkQl4BZgL9G3ndvuQgtGbwCnAUcA9QAA3\nA72ydCtl614H3svSHgw8mq0flaXbHDgmW/cAsC+paadp+8tbKMN4oL5kee8s7Z2kXxOHAX/P1h0+\nn+0GAf8B6knXCw4BzgdmAR8CqzXb9kPSr4exwAjgQqCB1HTSq43zNjYrz+BsX2+TNT2WpHkauBoY\nlqU9qeS9n2Tr/pYd3yHAn7O/xYsl531fUjt4Qza/eUn5ZwIfAceT2tS/1jwvoHd27hqBodm6rwKf\nZuXrV+3PrqfqT1UvgKeSP0b6j/1uO7dZBJiSBaLFmr33+ywo/DBbbgrGM4HlStItlO3jjZJ18wTu\ndgbzO4DppQEV6A/8E7h4Pts1Bdg9m+1/qyyY3dds2wCOaZb2smz91m2cu9Jgflg2P6Tk/dWydbu1\nEmAnZUG2+RfA1VnaDVo7zmblP6jZ+i/kla37cnY+/w9YODuPM4DVq/259VQbk5tZaks97e9htC0p\noJ8fEZObvXdy9rpns/XjI+LtpoWImEmqOS7Tzrzn5y3SRcWLJG0gSRExKyLWi4jDWtog60GzO/BC\nRFxf+l5EPATcC2wtaYlmm/652fLE7LU9x3MT8za17EWq9d/VPHFENADLATtFxGe9CLKL1J9kiwPK\nzLvFJpxm+b0GHA2sCvyD9EvpiIh4qcw8rOAczGvLO8Bikvq2mfJzX85e/938jYh4h1TjXrnZW++3\nsJ/ZpNpmpYwiBZ1DstcPJF0taXdJrXXzW4JU65znWDLPk7oIrlSyrjEiPmyWbnb2WvbxRMT7pJ5C\ne5SUby/gloiY3co2s4FNJF0m6UFJbwKTSV1Iobz/XwF8UGYZryD94lkVuDMixpaznfUMDua15SFS\nANpsfokk3ZYFkP60/TfszefBrck83e8q4Au/KCLi3YjYiNT+fhqpeWBP4C+kWnBL2jqWpjxKj6eS\nfWuvJzW5bCZpbWBt4NrWEkv6PXA3sCnwEnA2qUdMe7ouNpbW7OdH0mLA+tniFs0vCFvP5mBeW67L\nXg9tLYGkDYFdgQ0jYhbpQh/AWi2kXR4YSLowWglNvU76tfDeF5o0JH1V0sYR8VhEnBwRWwDLki62\n7i5pjRb28SGpXXieY8msRfoiyuPmJUhfNA3AHqRa+Uekpp15SBpCqoFfBawXESMi4oKImED6hZGH\n0aQLxEcDCwJXZk1TZg7mtSQi7ie1n+4taZ5+5JKWJl1cg8/bw8eRAuBRWc2t1M+z179UqIiTgDnA\n10qDiKTNgVWapb0WuE3Swk0rsuaQl7PFeW7hz9qhbwHWlPSFdn6lYQy+QWrvb35toCJKmlp2JwX0\nv0REa0MNNAXs55u1ma9IupsUvvhrpYFO/H+TtC/pl80lke5BOIN0ofS4ju7TisV3o9WeHwF/BS6Q\ntA+pa+FkUq30AFJN+8SIuAPSjUSSjgD+APxT0mWkdvJdSRdH72DeC4QdEhGzJF1P6l53m6RbSe3x\nh5GaGVYtSf4L0i+NCVm/6mmk5qP9gFsj4pVWsvkZKUhdLWkb4ClgDVLb+8fA4ZU4lvm4Abg4m5/f\njVkTsvKcLGkgqTvlGsBBpF47kC5MN3kfkKRRwMPRjrt3s+aUC4E3gBOy1aeT7jU4TdK4iHi23P1Z\nQVW7O42neSdSM8aBwIOkLodzSP3CbwS+3so2W5Nq6VNJvSmeJvU3712SZiXK71rYYlrSBcqLsvLM\nIvUc2RX4LfN2vduNVNP9MEv7AukXRf/W8s3WLZ3l8d/s2P8LXAoMnl+ZS9bvn5V93zbO89gs3eCS\ndUuRfjW81+zcDWPe7oIbkZphJpO+rJ4HzgTWy9JeWpJ2PeBf2fGMa6P8n+VFqs2Pz5a3aZZuo6ys\nz+K+5j1+8tgsZmYF4DZzM7MCcDA3MysAB3MzswJwMDczK4Ca65o4d9JrviKbswUGzTNst1m3VD/n\n7U49AQraF3P6LPHlTueXF9fMzcwKoOZq5mZmXaqxodolqAgHczPr2RpaG7Ghe3EwN7MerYVneHdL\nDuZm1rM1OpibmXV/rpmbmRWAL4CamRWAa+ZmZt1fuDeLmVkB+AKomVkBuJnFzKwAfAHUzKwAXDM3\nMysAXwA1MysAXwA1M+v+ItxmbmbW/bnN3MysANzMYmZWAK6Zm5kVQMPcapegIhzMzaxnczOLmVkB\nuJnFzKwAXDM3MysAB3Mzs+4vfAHUzKwA3GZuZlYAbmYxMysA18zNzArANXMzswJwzdzMrADq/XAK\nM7PuzzXzYptbX8/JZ5zHO+++z5y5czn4R99nhcHLMuqsC4iA1VddmROPPZTevXvz+z9dz9/ueZAB\nCy3IAft8l2Ff35S33nmPkaedQ0Sw7DJLMeqEo1igf/9qH1a3tMnGX+PMM07km9t+r9pFKbQll/wS\nTzx2FzvstDcvvfRqtYvTddxmXmy3330/iy48kF+d8hOmTJ3Gdw84grVWW5WjD96fjdZfh5GnncMD\njzzGCoMHccc947lmzG8B2PeQ49h0w/U456LL2fPbO7Hzdltz4213cdW1N3Pw/t+v8lF1Pz8+/lD2\n2WcPPpn5abWLUmh1dXVccvGv+XTWrGoXpetVqGYuqQ9wBbAS0A84Dfg3MBYI4Dng8IholHQqsDNQ\nDxwTEU9IWrWltOXm36siR1FA22+9JUcO3++z5brevTnv9JFstP46zJ07l0kfT+ZLiy/Ga//5Lxt/\nbV369etLv359WWHwcrz8yuu8+p83GbLZRgB8bd21eOrZ56t1KN3aq6+9wff2HF7tYhTeWb8+mTFj\n/si777xX7aJ0vcbG8qf52xf4KCK2BHYELgTOBU7K1gnYTdIGwFBgU2Bv4KJs+3nStucwcgvmkk5q\ntnxmXnnlYcEFF2ChhRZk5sxPOHbk6Rw5fD969+7NO++9z277HsLkKdNYeYXBfGWVlfjHM/9i5sxP\nmDJ1Gs88928+nTWLNb6yCuMfeQyA8Y88xqef9sAaTwXcfPPfmDu3GLdb16r9frgnkyZ9zLh7Hqx2\nUaojGsuf5u8G4OSS5XpgQ6DpxN4JbAMMAcZF8iZQJ2nJVtKWreLBXNJBkh4Ffizp79n0OLD9fLYZ\nIWmipImXX3VNpYvUYe++/yEHHPkzdt3hG+y83dYADFpmaf523e/Z89s7cdYFY1hlpRX4/h7f4pAf\nn8xvfncZ6661Oosusgg/OWI4DzzyOAcfdxJSLxZddOEqH41Zyw7Yfy+2+eaW3HfPDay33tqMveJ8\nll56yWoXq+vU15c9lcaqbBrRtJuImBER0yUNBG4ETgIUEZElmQ4sAiwMTC0pQdP6ltKWLY828z8B\n9wIjgdOzdY3AB61tEBFjgDEAcye9Fq2l60qTPp7MiGNHMvK4Q9lso68BcMRPR/GTI4ez4vLLsdCC\nC9CrVy8+njyFKVOn8sdLzmH6jJmMOHYkX/nyitw+7gEOPXAfVl91ZcZecxNbbLxBlY/IrGVbf3OP\nz+bvu+cGDjviZ7z//odVLFEXi/JDTmmsaomk5YGbgYsj4mpJZ5W8PRCYAkzL5puvb2xhXdnyCOZD\ns9cbgdVL1q8JjMshv1xcdtV1TJs+g9Fjr2H02PRr4agRP2Lk6efSp66O/v378YufHcNiiy7CW++8\nx14HHUWfPn04/vCD6N27NyuvMJiTzziPvn37sOrKKzDy+MOrfERm1qIK9WaRtDQpxh0REfdlq5+W\nNCwixpPa0R8AXgHOknQ2MBjoFRGTJLWUtvz8ox3fSmXtUPpDK29FRBzY1va1UjMvsgUGbVntIphV\nRP2ct9XZfXz655PLjjkL7PPLVvOTdD6wF/BiyeqjgQuAvsALwPCIaJA0ihSwewHHRsQjklYDLmue\nttyyVTyYd5aDef4czK0oKhLM/zSy/GC+7+mdzi8vufUzl/Quqb+kgMWB1yJizbzyMzPrkIayK781\nLbdgHhHLNs1LWhEYlVdeZmYd5jtAyxcRb0haoyvyMjNrFwfz+ZN0DamZBWAQ8H5eeZmZdZgH2mrT\n6JL5WcDEHPMyM+uQaCxGn4s8x2b5F6lGviKwBvDTHPMyM+uYyo3NUlV51sxvBF4G1iHVzD/JMS8z\ns44pSG+WXEdNjIhDgJeAbYHF8szLzKxDXDNvm6T+wEKkC6ED8szLzKxDajxIlyvPmvlFwDGksQr+\nyxdvcTUzqw0R5U81rOI1c0mnR8RIoD4ifpWtuyEiplU6LzOzTitIzTyPZpbdJL0DHJmNIgaApKbh\nI83MakdBuibmEcwPALYjPQNv2TbSmplVV0F6s1Q8mEfEk8CTksaRerKsSBpka2al8zIz66xwM0ub\nViQ9kaMOuF5SRMRpOeZnZtZ+BWlmybM3y7HAZsAk4DRg9xzzMjPrmMo90Lmq8qyZR0TMzmrkIcnN\nLGZWewpSM88zmD8k6WpgsKTRwJM55mVm1jH1vgDaKknrAg3ABsAfgSkR8bs88jIz65Qabz4pV8Xb\nzCV9D7gCeIM0UuIUYLik3Sqdl5lZpzVG+VMNy6NmfjQwtLQroqSxwG3ArTnkZ2bWYe6a2Lr65n3K\nI2K6pGI0TJlZsdR4jbtceQTz1r7mch1u18ysQxzMW7V21oullIC1csjLzKxzfDt/q/ZsZf3oVtab\nmVVNUZ4BmsfYLA9Wep9mZrlxMDczKwD3ZjEzK4CC1Mzdw8TMerYK3zQkaVNJ47P5pSTdKukhSRMk\nrZKtHy5poqTHJO2SrVtC0jhJD0u6TtKC7TkMB3Mz69GiobHsqS2SfgpcDvTPVp0F/DkitgJOAtaQ\ntAxwFPB1YHvgTEn9gFOAqyNiS+Bp4OD2HEfNNbMsMGjLaheh8Bbq27/tRNYpM+fMqnYRrFyVbWZ5\nFfgOaUwqSAH7WUn3Av8h3SH/TWBCRMwGZkt6BVgXGAKckW13ZzZ/XrkZu2ZuZj1aNEbZk6QRWfNI\n0zTiC/uKuAmYW7JqJWByRGwDvAmcACwMTC1JMx1YpNn6pnVlq7mauZlZl2pHzTx7KH17Hkz/EWlc\nKoC/AqcDE4GBJWkGkgYknJbNf1qyrmyumZtZz9bYjqn9HgF2yua3Ap4HngC2lNRf0iLAmsBzwISS\ntDsCD7cnI9fMzaxHi/pc+5kfD1wu6VBSE8oPImKypAtIwboXMDIiZkk6DbhS0nDS4zZ/0J6MFFFb\nfSzr+i5XWwUqIF8AzZ8vgHaN+jlvq7P7mLLX1mXHnEWve6DT+eXFNXMz69E8NouZWREU425+B3Mz\n69lcMzczKwLXzM3Mur+or3YJKsPB3Mx6tHDN3MysABzMzcy6v6LUzNt1O7+k5fMqiJlZNURj+VMt\na7NmLuko0sAviwIHSLorIo7LvWRmZl0gGmr2ps52KaeZ5fvAUOAuYG3gvlxLZGbWhWq9xl2ucoJ5\nAMsC70dESFo85zKZmXWZaOw5NfMHgIeA70s6D7gp3yKZmXWdHlMzj4iRwEgASU9GxNw2NjEz6zYi\nCl4zl/QoqYml+XoiYotcS2Vm1kV6Qs187y4rhZlZlTQWvTdLRLwBIGk54NfAksCNwLPAG11SOjOz\nnBXlAmg5Nw2NAa4A+pIuhJ6fa4nMzLpQNKrsqZaVE8z7R8T9QETES4Cfh2VmhRFR/lTLyumaOFvS\n9kBvSZvhYG5mBVLrNe5ylRPMRwBnA0sAPwYOzbVEZmZdqPBdE5tExFuSzgBWA56LiNfzL5aZWddo\nKEhvljbbzCWdBFwMfB34vaRjci+VmVkXiVDZUy0rp5llJ2BIRDRKqgMeAX6bb7HMzLpGT2oz/wBY\nEJhB6p74Ya4lMjPrQrXeS6Vc5dzOvxTwf5L+CawFfNRFZTMzy11PqJn7dn4zK7yGxnY9cK1mlXM7\n/6rA94A+gIBBwMFdUroaJokLf3cm6627FrNnz2bEIT/h1Vf/U+1iFUJdXR2jx/yGFVYYTENjA0cd\ncSL9+/XjvPN/SX19A6+88jpHHv6/RFF+H1dZT/8sF+VjVM5X0lXZ6xBgZeBL+RWn+9httx3o378f\nQ7b6FieOPJPfnHVKtYtUGNttP4y6ut5st833OOvM33HKqcdzwolH8etfXcgO2+1Fv3592X6Hratd\nzMLo6Z/lxlDZUzkkbSppfDa/vqSHJY2XdLekpbP1wyVNlPSYpF2ydUtIGpelv07Sgu05jnKC+ScR\ncSbwVkTsDyzdngyKasgWm3D3uAcAePyJp9hwg3WrXKLieOWV16mrq0MSAxcewNy59Tz7z+dZbLFF\nABgwYCHmzq2vcimLo6d/livZNVHST4HLgf7ZqvOBIyNiGPAX4ARJywBHkbp7bw+cKakfcApwdURs\nCTxNO1tAygnmyjIfIGkhoKzHxknqLekgST+XNEzSEu0pWK0buPAApk2d/tlyQ0MjvXv3rmKJimPm\njE9YYYXlmPjUPVzwuzMYfcmVvPrqG5z1m1N48qlxLLXUEjzy8GPVLmZh9PTPcoXHZnkV+E7J8t4R\n8Uw2X0caDmUTYEJEzI6IqcArwLqk1o+7srR3Atu05zjKCeY/B3YH/gS8nmVSjkuBFYHtgIF83lwz\nD0kjsp8cExsbZ5a5++qaPm0GAwYO+Gy5V69eNDQ0VLFExXHYEQdw370Ps+HXtuHrm+/C6Et/w6/P\nOpkdtt+bjTfYjmuuuZnTzzyx2sUsjJ7+WW5PM0tprMqmEaX7ioibgLkly+8CSNoCOAI4D1gYmFqy\n2XRgkWbrm9aVrc1gHhEPRcQlEXFbRCwVET8uc9+rRMQpwKcR8df5FSwixkTERhGxUa9eC5W5++qa\n8OiT7LjDNwDYdJMNeO65F6pcouKYMmUa06almuLkyVOo61PHtKnTmT5tBgDvvfsBiy7ars+5zUdP\n/yw3NPYqeyqNVdk0pq39S9oLGA3sHBEfAtNIFdwmA4EpzdY3rSvb/PqZv0sLj40DiIhB5ey7qWlF\n0kCgIA9nSm655U62+eZWPPzgrUjioOHHVrtIhXHxhVdw0SW/4s5x19K3Tx9+Meps3nrrHa4Yez71\nDfXMnTOXo45wzbxSevpnOc/OLJL2JbV9D4uIj7PVTwCnS+oP9APWBJ4DJpDuuB8L7Ag83K688ure\nJWko6cEWywL/BY6JiHva2q6u73IF6ShUuxbq27/tRNYpM+d4pOiuUD/n7U7f8fP3ZfcoO+Zs8e5N\nbeYnaSXgWtIFzg+BN/m8lv1gRJwqaThpRNpewBkRcVPW0+VKUq18EvCDiCi73Tm3YP5ZBtKSwKQo\nMyMH8/w5mOfPwbxrVCKYT1jmu2XHnK+/d2PN3i5aztgs7VIyDEDz9UTEFpXOz8ysM4rS/ltWMJe0\nMKlnymtlVPs9DICZdRtBzVa226XNYC7pu8DILO31kiIiTmstfckwAINJ3XDWAl4GetZVFTPrFupr\nfJzycpXTz/xYYDNSg/xppD7n5bgM+CPpIsCVwO87UkAzszwFKnuqZeUE88aImA1EdhGz3Kur/bO+\n6VMi4hbSQF1mZjWlsR1TLSsnmD8s6RpgsKTRwJNl7rtO0joA2at7qZhZzSlKzbycBzqfKGkH4Cng\nhYi4vcx9HwVcIWkQ8DapT6WZWU2p9Rp3ucq5ALpfNvs+sLik/SKi1XFWmkTE08DGnSyfmVmuGmq8\nxl2ucromrpm9Clgf+Jj5DJrVRNKpwOHAZ2OVljkMgJlZlynIU+PKamb536Z5SQLKbWbZBVgxIj7t\nYNnMzHLX2FNq5pL6liwuS3raUDk+oGQoSDOzWlSUnhnlNLO8RDpeAZ8Cv5lf4qznS5CeSPS0pOey\nZSLiB50qrZlZhfWYC6DAyRHxp3bsc3RHC2Nm1tUa1UOaWYDhpKcMlesRoDdpCMi9SDX63sAdwDfa\nW0AzszwV5ZlK5QTzfpKeJjW3NEKbzSUHAicCy2TbKNuuXQOtm5l1hR7TmwU4oT07jIjLgMskHRgR\nV3SsWGZmXaPwvVkkXRcRe0XEgx3c9z2SbqBk1MSI+E8H92Vmloui9GaZ39gsS3Zy32PwqIlmVuMa\nVf5Uy+bXzLKKpDNaeiMiynmabv+IuC2bv0WSxzM3s5rTE7omfkK6gNnhfUtaJyL+1TR6oplZrWmo\n8Rp3ueYXzN+LiCs7stPsMXP/Sxo1cVngHVIXRzOzmtITaub/6MgOJR0BHE8aYOvIiLirI/sxM+sK\nhQ/mEfHjDu7zB8DqwMKkC6AO5mZWswryCNCy+pm316yImANMajZIl5lZzSl8zbxCCvKdZ2ZF1ZNu\n52+vtSVdTQrkTfOAR000s9pT6/3Hy5VHMN+zZN4jKJpZTXMzSys6cfu/mVmXczA3MyuAoozN4mBu\nZj1apdrMJfUhjUO1Eum66nDS/TZjSd8ZzwGHR0Rj9sD7nbP3j4mIJzqbv4O5mfVoFezNshNQFxFb\nSNoWOB3oA5wUEeMljQZ2k/QGMBTYFFgeuAnYuLOZO5j3QDPnzKp2EQpv+YFLVLsIVqbGdjS0SBoB\njChZNSYixmTzL5PGpOpFumlyLrAZ0HQd8U5gO9KYV+MiIoA3JdVJWjIiPuzMcTiYm1mP1p4LoFng\nHtPK2zNITSwvAksAuwBbZUEbYDqwCCnQf1SyXdP6TgXz+Y1nbmZWeNGOqQ3HAndHxGrAeqT289K7\n4AcCU4Bp2Xzz9Z3iYG5mPVpjO6Y2TAamZvMfk9rLn5Y0LFu3I+lZyBOA7SX1krQC0CsiJnX2ONzM\nYmY9Wr0q1jnxPNKw3w+TauQnAhNJz0TuC7wA3BgRDVmaR0kV6sMrkbmDuZn1aJUK5RExgy/eAd9k\naAtpRwGjKpQ14GBuZj2c7wA1MyuA9nRNrGUO5mbWoxUjlDuYm1kP52YWM7MCaChI3dzB3Mx6NNfM\nzcwKIFwzNzPr/lwzNzMrAHdNNDMrgGKEcgdzM+vh6gsSzh3MzaxH8wVQM7MC8AVQM7MCcM3czKwA\nXDM3MyuAhnDN3Mys23M/czOzAnCbuZlZAbjN3MysANzMYmZWAG5mMTMrAPdmMTMrADezmJkVgC+A\nmpkVgNvMzcwKwM0sZmYFEAW5ANqr2gXorurq6hj7hwsYf/9feHTC7eyyy7bVLlIhSeKiC3/FIw/d\nxn333MAqq6xU7SJ1W3V1dZx7yelcf/sfuOWeP7PNDkNZceXluf6OsVx/+x/45dkjkfRZ+hVXXp67\nHrmpiiXuGg1E2VMtc828g/b5wXf46KPJ7H/AUSy++GJMfOJubr/9nmoXq3B2220H+vfvx5CtvsWm\nm2zAb846he/scWC1i9UtfXvPnZn88RSOO3Qkiy62CLePv44X/vUS55xxIY9PmMhpZ5/Etjttzbg7\n7mf3PXdh/4N/wOKLL1rtYueu0s0skpYC/gFsC9QDY0lPp3sOODwiGiWdCuycvX9MRDzR2XxdM++g\nG2+6nVNHnfXZcn19fRVLU1xDttiEu8c9AMDjTzzFhhusW+USdV9/u3Uc55550WfLDfUNfHX9tXh8\nwkQAHrzvEYYM3RSAqVOmsfeuB1WlnF0tIsqe2iKpD3Ap8Gm26lzgpIjYEhCwm6QNgKHApsDewEUt\n7au9cg3mkr4t6QRJu+SZTzXMnPkJM2bMZMCAhbj+2jGcUhLYrXIGLjyAaVOnf7bc0NBI7969q1ii\n7uuTmZ8yc8YnLDRgQS7+wzmcc8aFlLSqMGPGJwxceCAA9497iE8/+bSVPRVLI1H2VIazgdHAO9ny\nhsCD2fydwDbAEGBcJG8CdZKW7Oxx5BbMJV0OfB+YBewn6bz5pB0haaKkiY2NM/MqUsUNHjyIe++5\ngT/9+UauvfaWahenkKZPm8GAgQM+W+7VqxcNDQ1VLFH3tuygpbn61su5+frbue2mO2ls/DxADRiw\n4Be+OHuKaMe/0liVTSOa9iNpf+DDiLi7ZPeKz6v004FFgIWBqSVpmtZ3Sp5t5utExKbZ/PmSHmst\nYUSMAcYA1PVdrravMmSWWmoJ7vzb1Rx99Enc/8Aj1S5OYU149El22Xlbbrzxr2y6yQY899wL1S5S\nt7XEkotz1U2jOfWEM/n7Q6mJ9t/PvsimX9+IxydMZOg3h/DoI09WuZRdrz2385fGqhYcCISkbYD1\ngauApUreHwhMAaZl883Xd0qewfwVSStHxOvZBYE3c8yry/3shCNZbNFFGHni0Yw88WgAdt71h8ya\nNavKJSuWW265k22+uRUPP3hMmlmgAAAMrElEQVQrkjho+LHVLlK3ddix/8MiiyzMkceP4MjjU4Xy\nFyeexalnnkCfvn145eXXufO2nncRv1IXQCNiq6Z5SeOBQ4DfSBoWEeOBHYEHgFeAsySdDQwGekXE\npM7mr7z6WEp6DViOFMSXA2aTmlwiIga1tl13qZmbzc/yA5eodhF6hNc/+qfaTjV/my+3ddkx59G3\nHygrv5Jg3ghcBvQFXgCGR0SDpFGk4N4LODYiOv3zPreaeUR8Oa99m5lVSh4V2ogYVrI4tIX3RwGj\nKplnbsFc0q7AAUD/pnURsVNe+ZmZdYRv52/b2cDBwOQc8zAz6xQPtNW257NGfzOzmtUQxRgEN89g\nfqukR0mN/gBEhO/DNrOaUpSBtvIM5kcBZ1GB/pNmZnlxm3nb3ouI63Lcv5lZp7nNvG2fSroLeJo0\nYhgRcWKO+ZmZtVujm1na9Ncc921mVhGumbft9Rz3bWZWEe7N0rZDs1cBawP/AR7KMT8zs3ZzM0sb\nIuL7TfOS+gLX55WXmVlHuZml/fl4rBYzqzmumbdB0rukXizK8vltXnmZmXWUa+ZtiIhl89q3mVml\nNEQxnlxV8cfGSVpI0nGSdpe0rKR7JI3PHmJqZlZTKvlA52rK4xmgVwJfArYFHgauAU4HLsghLzOz\nTqnwA52rJo9mlqUj4rsAkv4ZEVdk8z/NIS8zs06p9Rp3ufII5nNL5j8qme+dQ15mZp3i3iytW07S\nCFIvltL5Vp/7aWZWLe7N0rqrgWVbmL8mh7zMzDrFt/O3IiJ+Xul9mpnlxW3mrSi5WagfsCDwX2A5\n4MOIWKnS+ZmZdUZR2swr3jUxIpaNiEHAncBqEbEa8BXg8UrnZWbWWUXpZ57n2Cxfjoj/AkTEO5JW\nyDEvM7MOqfX+4+XKM5j/W9IfgSeAzUk3EJmZ1ZRar3GXK89gPgLYEVgLuDYibssxLzOzDilKb5Y8\nbudvshCpRr4GUCdp1RzzMjPrkMaIsqdalmcwvwJ4DVgNeA/4fY55mZl1SFEugOYZzL+UjcsyNyL+\nTroL1MyspkQ7/s2PpF6SRkt6NBsptktbI3J90pCkNbLXwUAxBg02s0KpYI3720D/iNhc0mbAOcBu\nldp5W/IM5kcBfwDWBG4EDssxLzOzDqlgW/gQ4C6AiHhM0kaV2nE58nzS0HOSdgBWBF6LiBnlbFc/\n5+1u1xwjaUREjKl2OYrM5zh/PfUctyfmZAMHjihZNabknC0MTC15r0FSXUTUV6CYbZctr0Z9SXsA\nJ5G+MK4HIiJOyyWzKpM0MSK69Fu4p/E5zp/PcedIOhd4LCKuz5bfiojBXZV/nhdAjwM2AyYBpwG7\n55iXmVm1TQB2AsjazP/VlZnn2WbeGBGzJUVEhKSZOeZlZlZtNwPbSmrqvXdAV2aeZzB/WNI1wGBJ\no4Enc8yr2npcO2MV+Bznz+e4EyKiETikWvnn1mYOkF0AXQd4ISJuzy0jM7MeLreauaSlSGOzrA4s\nLWlCREzOKz8zs54szwug1wEvACeQbuv/Y455mZn1aHkGcyJidET8MyIuBgbkmVdnSRom6YPsNtwH\nJf1d0p7zSb+JpOclndnOfMZK2kFSf0n/0/mSdw/Nzu8Dkh6TdOR80o+Q1Cfb7toy81hJ0mOVK3Vt\nkPQzSfdKGifpbkkb5pDHe9nrKEkvZ3+nRyT9RdLASudnlZdnMH9R0r6SBknaFfhI0mqSVssxz866\nPyKGRcRQYDvgBEnrt5J2O2B0RPxvB/NaBugxwTzTdH63BoYCx0tatJW0JwK9u65otUnSWsC3gG0j\nYjvSL90rcs723OzvNAR4ETg45/ysAvLszbJGNv2YNC7LNOBS0vNBv5FjvhURETMkXQp8V9JewFak\nL79zgTdIgXiOpLdIQedwPh9M7LvAV4FDImJvSDWfiFimJIuRwFqSTomIX3TJQdWWgaTPxTrZr5sG\nYBYwHNiW9GV3LfBb4CuS7gSWAv4aEaMkfQ34XbPtPiNpKHB69v6rwMERMbcrDqzCPgBWAA6UdFdE\nPJP9KhxPCrRrkD53e0XEe9m5/OyzGhE3SFoHuCBL9xFwIDCD1HtlbdL56ddK/osBT8EXP8PZr6XR\nwErArsACwLLA+aTxSL4K/DgibpX0GumxkasAzwH/k/X8sEpqz/CPZQ4RuQHwNNAX+A7wPvAy8K1K\n51Xhcg8jPUSjdN1uwEtN64H+wDPAosAoUrCGVItcMJu/FNin+f6A97LXscAOpP8Ej1X7uLv4/H4A\njAfuB+4m3WAxEVi/5HzfmM3/Jzvfw4Dn+fwB4ZOy9+fZrumckoLWy8BS2fu/BIZX+xx04txtQKqN\nv0kK4Htk53G/7P3DSMF6x1Y+q48Ba2XrDyJ9ye0C/DlbtwIwO5sflZ278cCzpOtdy5V+hrP5a7O/\nzf7AuGzd3qSgLWBr4JZs/Rxg1Wz+euA71T6nRZzyqJmfDvwoIuZIOi37gL1CesBzd3va0IrAn4Ef\nZjUhgD7Z+lIfAFdKmkGqKT3awr663ZgzObg/sl8qTSRdHhHPZIsPAb9qYbvnImJ2lr5pnItB89lu\nSVIt8XpJkGqN4ypzCF0rG0Z1WkQcmC1vBPyN9IyA+7Nkfyd9ob0FbNjCZ3VN4OLsXPQhBeu1SY90\nJCLelPTfkmzPjYjRWX4HAVcC2zQvWsn809nrFFI35JA0mfSFAvBmRLxSUtbV23karAx5tJn3iohn\nJQ0CFoqIpyJiGnSvp6ZmF32GkwbOeSAihpGah64n1Vaa0i0C/JxUK/kf4FPSB30WKaAgaUVg8WZZ\nNJLzBehu4h1J62bzQ0mBBr54flr67LS2HaQhJN4Cdsv+bqcDD1Sy0F1oXeASSU2B8WXSZ7IBaLoQ\n+nXSr5cXafmz+hKpFj8M+ClwR5Z2c4Ds/+pyreT/JulXNkAfSQMk9SV9GTRp6//2cpKamhibymoV\nlkfNvOk/4A7AvQCS+lHjvVky38hqNQ2kc3Mq6RbdcyQ9TDqGmyNielbLgXQtYAKpXXEmMBkYROqK\nOUXS46Qumq83y+sDoK+kX0fECbkeVW0bDlyodELrSc0AkB4A/jfSF2V7tiMiGiUdDdwhqRfpb7Rf\nTuXPVUT8RdKawOPZL79ewE+AY4D9JR1H+tz9EPgYGNbCZ/VQ4CpJTReUD4qIlyUNyT6fb5C+AJsc\nJ2lv0nldEDg6W/9bUpPNa9k25ZpN+lstn23/13aeBitDxe8AlXQC6er78tnrdOASUo2hXd34zKxl\nWaXjkIh4sdplaUsLF/8tBxWvmUfEryXdBnwQER9JWgW4JCJurnReZmaW5Do2i5mZdQ1fgDMzKwAH\nczOzAnAwNzMrAAdza1F7B8ZqY1+/krS/pPUlnTKfdLtnfZ7L2ecOksa2UOZWB+XKytDSTUmdSmtW\nC/Icm8W6v8/u2MzuFXhJ0h8jYkpHdpbdsfnMfJIcTXpSyzsd2b9ZT+aauZWraWCs+qy2fkM2LGs/\nSb+X9FA2ZOowAEl7SHpa0jjSg72/UHOWdJCkiVmaUZJ2BtYn3dzSV9KRkh5VGor4qGybNbN19wKH\nzq+wko6QdL+khyXdnt21CLC5pPskPZnliaShWdkflHSFpD4l++kv6bbsvSeajs+s1rhmbvPTdEds\nIzAXODLSaJIAV0fEzdndhZMi4iBJXyKNk7I2cBawCemuxDtKd6r0FKqfkW5Vnw2cAzxIqrUfAqwK\n7AUMId0qfq+ku0kDZp0SEfdkN6et2VKhs7s+vwRsk90Nejewcfb2TGBn0vgtj0u6C7gMGBIRH0j6\nJWnwqKYRFlchjeC4DWnUxloewtl6MAdzm595BsYq8VL2ug6wpaRNs+U6SUuTBof6CEDpaeWlvkwa\nPOvTbPnYLF3T+18lDRB1X7a8GCnAfzY4FGkIhRaDeRbA5wDXZLfADyYNMAXwSKSbKz6QNBVYgpYH\n5Xo129fzki4Crsn2cUEr58OsqhzMraOaxqN+EXgrIs6QtABpnPbJwCKSloyID0m14rdKtn0VWENS\nv4iYLelGUnt50+BaL5EGY9oxG4HvWOBffD441F18XtOeRzYA17cjYlNJCwL/4PNR/jbO0ixDGr+k\ndFCuqZK+RRrre4Us3TrAwIjYWdKypFH//HByqzkO5tZZlwKXSXoQWBi4OBv++ADgbkkf83mTBQAR\n8aGkXwMPSgrSAyfezmrwV5Ge4nQf8Eh24fUJ4G3SuN3XSfoJ8CFpZMqWvALMlDSR1IzzLmnwM4AF\nJN1PCuQHR0RDK4NyrZCl/z/gVEn7kcblbrU3jlk1+XZ+M7MCcG8WM7MCcDA3MysAB3MzswJwMDcz\nKwAHczOzAnAwNzMrAAdzM7MC+H+lOo3ltJIjCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2973a4c1be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make a prediction\n",
    "yhat   = model.predict(x_test)\n",
    "\n",
    "# Print Accuracy value\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# Build Confusion Matrix\n",
    "build_confusion_matrix(y_test, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
